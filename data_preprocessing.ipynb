{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "import torch.nn.functional as F \n",
    "from operator import itemgetter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "data = pd.read_excel('data/training_set_rel3.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert resolved grades to percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grade columns\n",
    "data['grade'] = np.nan\n",
    "\n",
    "# Essay set #1: 2-12 grade range\n",
    "data.loc[data.essay_set == 1, 'grade'] = (data.loc[data.essay_set == 1, 'domain1_score'] - 2) / 10\n",
    "\n",
    "# Essay set #2: 2 domains with 1-6 and 1-4 grade ranges respectively\n",
    "data.loc[data.essay_set == 2, 'grade'] = ((data.loc[data.essay_set == 2, 'domain1_score'] - 1) + (data.loc[data.essay_set == 2, 'domain2_score'] - 1)) / 8\n",
    "\n",
    "# Essay set #3, #4: 0-3 grade range\n",
    "data.loc[data.essay_set == 3, 'grade'] = data.loc[data.essay_set == 3, 'domain1_score'] / 3\n",
    "data.loc[data.essay_set == 4, 'grade'] = data.loc[data.essay_set == 4, 'domain1_score'] / 3\n",
    "\n",
    "# Essay set #5, #6: 0-4 grade range\n",
    "data.loc[data.essay_set == 5, 'grade'] = data.loc[data.essay_set == 5, 'domain1_score'] / 4\n",
    "data.loc[data.essay_set == 6, 'grade'] = data.loc[data.essay_set == 6, 'domain1_score'] / 4\n",
    "\n",
    "# Essay set #7: 0-30 grade range\n",
    "data.loc[data.essay_set == 7, 'grade'] = data.loc[data.essay_set == 7, 'domain1_score'] / 30\n",
    "\n",
    "# Essay set #8: 0-60 grade range\n",
    "data.loc[data.essay_set == 8, 'grade'] = data.loc[data.essay_set == 8, 'domain1_score'] / 60\n",
    "\n",
    "# Remove ungraded essays\n",
    "data = data[~data.grade.isnull()].reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert essays to word embeddings sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained Word2Vec model\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce the dimensionality of word embeddings\n",
    "target_dim = 50\n",
    "dictionary = list(w2v.key_to_index.keys())\n",
    "emb_matrix = np.zeros((len(dictionary), 300))\n",
    "\n",
    "for i, word in enumerate(dictionary):\n",
    "    emb_matrix[i,:] = w2v[word]\n",
    "\n",
    "pca = PCA(n_components=target_dim, random_state=0)\n",
    "emb_matrix_pca = pca.fit_transform(emb_matrix)\n",
    "\n",
    "w2v_pca = {}\n",
    "for i, word in enumerate(dictionary):\n",
    "    w2v_pca[word] = emb_matrix_pca[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12977/12977 [00:18<00:00, 707.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# Create list of embedding matrices\n",
    "emb_essays_w2v = []\n",
    "\n",
    "for essay in tqdm(data.essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    emb_essay = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence[:-1]:\n",
    "            try:\n",
    "                emb_essay.append(w2v_pca[word])\n",
    "            except:\n",
    "                if word != ',':\n",
    "                    emb_essay.append(np.zeros(target_dim))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        emb_essay.append(w2v_pca['</s>'])\n",
    "    \n",
    "    emb_essays_w2v.append(torch.tensor(np.array(emb_essay).T).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max essay length\n",
    "essays_lengths_w2v = [mat.shape[1] for mat in emb_essays_w2v]\n",
    "max_essay_length_w2v = np.max(essays_lengths_w2v)\n",
    "max_essay_length_w2v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained GloVe model\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12977/12977 [00:04<00:00, 2859.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create list of embedding matrices\n",
    "emb_essays_glove = []\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|[^\\w\\s]\")\n",
    "\n",
    "for essay in tqdm(data.essay):\n",
    "    emb_essay = []\n",
    "    tok_essay = tokenizer.tokenize(essay)\n",
    "\n",
    "    for word in tok_essay:\n",
    "        try:\n",
    "            emb_essay.append(glove[word.lower()])\n",
    "        except:\n",
    "            emb_essay.append(np.zeros(50))\n",
    "    \n",
    "    emb_essays_glove.append(torch.tensor(np.array(emb_essay).T).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max essay length\n",
    "essays_lengths_glove = [mat.shape[1] for mat in emb_essays_glove]\n",
    "max_essay_length_glove = np.max(essays_lengths_glove)\n",
    "max_essay_length_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max essay length (W2V abd GloVe)\n",
    "max_essay_length = max(max_essay_length_w2v, max_essay_length_glove)\n",
    "max_essay_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training/validation/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12977/12977 [00:03<00:00, 3518.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert essay_set variables and grades to list of tensors\n",
    "essay_sets = []\n",
    "grades = []\n",
    "\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    essay_sets.append(torch.tensor(data.essay_set[i]))\n",
    "    grades.append(torch.FloatTensor(data[['grade']].iloc[i].values)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a given fraction of data samples for each essay set\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 1 - val_frac - val_frac\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for i in range(1,9):\n",
    "    df = data[data.essay_set == i].sample(frac = 1)\n",
    "    n_samples = df.shape[0]\n",
    "    n_train_samples = int(train_frac*n_samples)\n",
    "    n_val_samples = int(val_frac*n_samples)\n",
    "    train_idx += list(df.index)[:n_train_samples]\n",
    "    val_idx += list(df.index)[n_train_samples:(n_train_samples+n_val_samples)]\n",
    "    test_idx += list(df.index)[(n_train_samples+n_val_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation subsets\n",
    "essays_w2v_train = list(itemgetter(*train_idx)(emb_essays_w2v))\n",
    "essays_w2v_val = list(itemgetter(*val_idx)(emb_essays_w2v))\n",
    "essays_w2v_test = list(itemgetter(*test_idx)(emb_essays_w2v))\n",
    "\n",
    "essays_glove_train = list(itemgetter(*train_idx)(emb_essays_glove))\n",
    "essays_glove_val = list(itemgetter(*val_idx)(emb_essays_glove))\n",
    "essays_glove_test = list(itemgetter(*test_idx)(emb_essays_glove))\n",
    "\n",
    "essay_sets_train = list(itemgetter(*train_idx)(essay_sets))\n",
    "essay_sets_val = list(itemgetter(*val_idx)(essay_sets))\n",
    "essay_sets_test = list(itemgetter(*test_idx)(essay_sets))\n",
    "\n",
    "grades_train = list(itemgetter(*train_idx)(grades))\n",
    "grades_val = list(itemgetter(*val_idx)(grades))\n",
    "grades_test = list(itemgetter(*test_idx)(grades))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad essays to set them all to the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding essays from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10379/10379 [00:03<00:00, 2721.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding essays from validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1295/1295 [00:00<00:00, 2582.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding essays from test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1303/1303 [00:00<00:00, 2049.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train_w2v, train_glove = [], []\n",
    "\n",
    "print('Padding essays from training set...')\n",
    "for i in tqdm(range(len(train_idx))):\n",
    "    # W2V\n",
    "    essay, essay_set, grade = essays_w2v_train[i], essay_sets_train[i], grades_train[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    train_w2v.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "    \n",
    "    # Glove\n",
    "    essay = essays_glove_train[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    train_glove.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "\n",
    "train_w2v, train_glove = ConcatDataset(train_w2v), ConcatDataset(train_glove)\n",
    "\n",
    "# Validation data\n",
    "val_w2v, val_glove = [], []\n",
    "\n",
    "print('Padding essays from validation set...')\n",
    "for i in tqdm(range(len(val_idx))):\n",
    "    # W2V\n",
    "    essay, essay_set, grade = essays_w2v_val[i], essay_sets_val[i], grades_val[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    val_w2v.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "    \n",
    "    # Glove\n",
    "    essay = essays_glove_val[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    val_glove.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "\n",
    "val_w2v, val_glove = ConcatDataset(val_w2v), ConcatDataset(val_glove)\n",
    "\n",
    "# Test data\n",
    "test_w2v, test_glove = [], []\n",
    "\n",
    "print('Padding essays from test set...')\n",
    "for i in tqdm(range(len(test_idx))):\n",
    "    # W2V\n",
    "    essay, essay_set, grade = essays_w2v_test[i], essay_sets_test[i], grades_test[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    test_w2v.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "    \n",
    "    # Glove\n",
    "    essay = essays_glove_test[i]\n",
    "    essay_pad = F.pad(essay, (0, max_essay_length - essay.shape[1]))\n",
    "    test_glove.append(\n",
    "        TensorDataset(essay_pad.view(1, essay_pad.shape[0], essay_pad.shape[1]), essay_set.view(1), grade.view(1)))\n",
    "\n",
    "test_w2v, test_glove = ConcatDataset(test_w2v), ConcatDataset(test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save padded data sets\n",
    "torch.save(train_w2v, 'data/train_w2v.pt')\n",
    "torch.save(train_glove, 'data/train_glove.pt')\n",
    "torch.save(val_w2v, f'data/val_w2v.pt')\n",
    "torch.save(val_glove, f'data/val_glove.pt')\n",
    "torch.save(test_w2v, f'data/test_w2v.pt')\n",
    "torch.save(test_glove, f'data/test_glove.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
